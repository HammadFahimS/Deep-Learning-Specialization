# Week 1: Improving Deep Neural Networks - Hyperparameter Tuning, Regularization, and Optimization

This repository contains the materials and assignments for **Week 1** of the **"Improving Deep Neural Networks: Hyperparameter Tuning, Regularization, and Optimization"** course from the **Deep Learning Specialization** by Andrew Ng on Coursera.

## Contents

1. **Gradient_Checking.ipynb**  
   This notebook explains and implements gradient checking, a method to verify the correctness of your gradient computations in neural networks.

2. **Initialization.ipynb**  
   This notebook focuses on different weight initialization techniques and their effects on the convergence of neural networks.

3. **Regularization.ipynb**  
   This notebook covers regularization techniques, including L2 regularization and dropout, to prevent overfitting and improve generalization in neural networks.

4. **readme.md**  
   This file provides an overview of the content and purpose of this repository.

## Learning Objectives

- Understand and implement gradient checking to debug backpropagation.
- Explore weight initialization techniques (e.g., random initialization, He initialization) to improve the performance and convergence speed of deep neural networks.
- Apply regularization methods to prevent overfitting and enhance model robustness.

## Usage

1. Clone this repository:
   ```bash
   git clone https://github.com/<your-username>/Deep-Learning-Specialization.git

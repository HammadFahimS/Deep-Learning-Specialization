# Week 2: Improving Deep Neural Networks - Hyperparameter Tuning, Regularization, and Optimization

This repository contains the materials and assignments for **Week 2** of the **"Improving Deep Neural Networks: Hyperparameter Tuning, Regularization, and Optimization"** course from the **Deep Learning Specialization** by Andrew Ng on Coursera.

## Contents

1. **Optimization_methods.ipynb**  
   This notebook covers various optimization techniques, including momentum, RMSprop, and Adam, to improve the efficiency and convergence of deep neural networks.

2. **readme.md**  
   This file provides an overview of the content and purpose of this repository.

## Learning Objectives

- Understand and implement optimization algorithms such as:
  - Gradient Descent with Momentum
  - RMSprop (Root Mean Square Propagation)
  - Adam Optimization
- Compare and analyze the performance of different optimization methods.
- Improve the convergence speed and stability of training deep neural networks.

## Usage

1. Clone this repository:
   ```bash
   git clone https://github.com/<your-username>/Deep-Learning-Specialization.git
